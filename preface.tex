\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}
\subsection*{Motivation}

Although the Moore's law\footnote{https://en.wikipedia.org/wiki/Moore's\_law} still applies and the performance of computers is now greater than it ever was, there still exist some tasks which require a lot of computer power. Despite parallelism, their accomplishment takes a non-trivial amount of time. Among these tasks are for example computations of some special prime numbers, processing large data sets and many others.

It's natural to ask, how we can speed up such computations. Obvious possibility is to get better hardware, however this is limited by technical progress and can be quite expensive. Widely used approach is to parallelize the computation, that is different parts of the job are run concurrently (in parallel) which results in reduction of the processing time. Generally, tasks can be parallelized on different levels. We will talk about parallelism on the whole computers level, i.e. each computing node is represented by a standalone machine. These nodes are independent and their physical distance can be arbitrary big. The only condition which has to be fulfilled is, that the nodes are connected together and form a network. It means that they can communicate and exchange data. This approach makes sense only with sufficiently big tasks, because the network communication is relatively slow and for many tasks it would be a bottleneck. There are also different ways how the general term "parallelism" can be understood. We will consider so-called data parallelism. That is, each computing node runs the same program, only the data differs. This is the main difference from the task parallelism concept in which different nodes can run completely different tasks.

Obviously, not all tasks can be parallelized easily or even are not possible to parallelize at all. On the other hand, there exist also jobs that can be split into smaller parts, each of which can be processed independently and the results then can be joined together in the end. Concurrent processing of such jobs is quite straightforward and allows to use of the data parallelism. These jobs are suitable for our framework.

\subsection*{Goal of the thesis} % (fold)
The main goal is to implement a robust framework which should be able to split the given job and  distribute the work among the participating nodes. Each node then processes the assigned parts and the results are eventually collected and joined together. The domain of suitable jobs is restricted to those that are described in the preceding paragraph.

The concept of parallelization has been known for many years and many different approaches have been introduced. Some of them are discussed in \hyperref[existing-solutions]{chapter 1}. Nevertheless, the great majority of them use some kind of hierarchy or need some control node, whose functionality differs from the others. Our main motivation was to provide solution, in which all the nodes are equivalent. Furthermore, each can initiate the process or serve as a computing node. There also exists one node which controls the process while the rest is working. But the point is, that every single node can become the initiator (master) or serve as a slave or even both at the same time. Our framework was implemented with the application for the video encoding in mind. However its essence is generic, so it could be easily modified to be used with an arbitrary job which fulfills the conditions mentioned above.

The purpose is to implement framework which conforms to the peer to peer paradigm as much as possible. We also want to keep the logic in the node which have demands to be processed in order to ensure better control and more deterministic behavior. Another important goal is to minimize occurrences of errors in the system and provide good ability to recover from them. The system is also intended to use personal computers as computing nodes, so the program typically runs alongside other applications.


% section goal_of_the_thesis (end)
\subsection*{Application to the video encoding}
Video files are basically sequences of images, so-called frames. When the file is played, the images are showed sequentially one by one. They are changed many times per second. so the illusion of smooth video is made. There are many different ways how to code the video into the digital form. Because of this, the need arises to convert the files from one format to another. The program which codes the analog video data to digital or decodes it the other way around is called codec. Some of the well known codecs are for example \textit{H.264}\footnote{https://en.wikipedia.org/wiki/H.264/MPEG-4\_AVC} or \textit{MPEG-2}\footnote{https://en.wikipedia.org/wiki/MPEG-2}. In order to convert the file, each frame has to be re-encoded, which takes quite a large amount of time. This makes the video encoding ideal task for our framework.

Before we use it, some more details have to be revealed. Because we usually want not only to watch the video but also listen to some sound, the video files are usually accompanied by one or more audio files. All of them are packed together in some container. When we want to convert the video, the container has to be opened, the video extracted, processed and stored again. The audio files are not our concern and we will simply copy them. Common container formats are \textit{avi}\footnote{https://en.wikipedia.org/wiki/Audio\_Video\_Interleave} or \textit{mkv}\footnote{https://en.wikipedia.org/wiki/Matroska}.

There is another important thing. In order to reduce file size and decrease the requirements of rendering the video, the video file actually contains only few frames which carry the whole information. These are usually referred to as the key frames. The rest of them contains only information how the frame differs from the previous one. This approach greatly reduces the file size and preserves good ability to recover from errors, e.g. when some part of the file is damaged. Consequence of this fact is, that we cannot split the video file at an arbitrary place. This is because if the chunk started with some non-key frame, the conversion would not be done properly.

Issues connected with video encoding are quite complex and used algorithms are very sophisticated and advanced. W edid not reinvent the wheel and used third party software for the video encoding, namely the ffmpeg and ffprobe from the FFmpeg\footnote{https://www.ffmpeg.org/} project. For the sake of simplicity, the program outputs always video files packed in the Matroska container. The choice of the codec is arbitrary, however it is restricted by the list of options. All of this software is free and published under the GNU General Public License\footnote{https://en.wikipedia.org/wiki/GNU\_General\_Public\_License} or GNU Lesser General Public License\footnote{https://en.wikipedia.org/wiki/GNU\_Lesser\_General\_Public\_License} in case of FFmpeg.

\subsection*{Terminology}
To prevent misunderstandings there is a short list of frequently used terms:
\begin{itemize}
\item \textit{master}(or \textit{initiating node, initiator}) is a node which has job to be done, i.e. has file to be encoded. It initiates and controls the process.
\item \textit{task}(\textit{job}) refers to work that should be done, specified by the initiator.
\item \textit{chunk} denotes one part of the split file, which is supposed to be encoded independently.
\item \textit{(computing) node} is one particular entity which communicates with the master and encodes chunks for it. Theoretically, there can be more such entities on one physical computer, because each node is determined by its communicating port and address.
\item \textit{neighbor} is called every node which is in the list of the node we are considering.
\end{itemize}

\subsection*{Thesis organization}
In the first chapter is given overview of the existing technologies and approaches and the description of our framework. Second chapter describes implementation details. Third chapter introduces some experiments and summarizes the results. The fourth chapter is dedicated to alternatives and possible future improvements. In the appendix is then described installation and use of the program.