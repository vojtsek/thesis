\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}
\subsection*{Motivation}

Although the Moore's law still applies and the performance of computers is now greater than ever, there still exists some tasks which require a lot of computer power and despite parallelism their accomplishment takes a non-trivial amount of time. Among these tasks are for example computations of some special prime numbers, processing large data sets and many others.

It's natural to ask, how we can speed up such computations. Obvious possibility is to get better hardware, however this is limited by technical progress and can be quite expensive. Widely used approach is to parallelize the computation, that is different parts of the job are run concurrently (in parallel) which results in reduction of the processing time. Generally tasks can be parallelized on different levels. We will talk about parallelism on the level of whole computers, i.e. each computing node is represented by a standalone machine. These nodes are independent and their physical distance can be arbitrary long. The only condition which has to be fulfilled is that the nodes are connected together by the network so they can communicate and exchange data. This approach can be used with big tasks only, because the network communication is relatively very slow and for many tasks it would be a bottleneck. There are also different ways how the general term parallelism can be understood. We will consider so called data parallelism. That is, each computing node runs the same process, only the data differs. This is the main difference from the task parallelism concept in which different nodes can run completely different tasks.

Obviously, not all tasks can be parallelized easily or even are not possible to parallelize at all. On the other hand, there exists also jobs that can be split into smaller parts, each of which can be processed independently and the results then joined together in the end. Concurrent processing of such jobs is quite straightforward and allows use of the data parallelism. These jobs are suitable for our framework.

\subsection*{Goal of the thesis} % (fold)
The main goal is to implement robust framework which is able to split given job, distribute work among participating nodes, which process the assigned parts in parallel, and eventually collect the results and join them together. The domain of suitable jobs is restricted to ones that are described in the preceding paragraph.

The concept of parallelization has been known for many years and many different approaches have been implemented. Some of them are discussed in \hyperref[existing-solutions]{chapter 1}. Nevertheless, the great majority of them uses some kind of hierarchy or at least some control node exists which has different functionality than the others. Main motivation of our work was to provide solution, in which all the nodes are equivalent, each can initiate the process or serve as a computing node. Even in our framework there exists always one node which controls the process while the other ones do the work. But the point is, that every single node can become the initiator (master) or serve as a slave or even both at the same time. Our framework was implemented with the application to the video encoding in mind. However its essence is generic, so it could be easily modified to be used with an arbitrary job which fulfills the above mentioned conditions.

The purpose is to implement framework which conforms to the peer to peer paradigm as much as possible. We also want to keep the logic in the node which have demands to be processed in order to ensure better control and more deterministic behavior. Another important goal is to minimize occurrences of errors in the system and good ability to recover from them. The system is also intended to use personal computers as computing nodes, so the program typically runs alongside other applications.


% section goal_of_the_thesis (end)
\subsection*{Application to the video encoding}
Video files are more or less sequences of images, so called frames. When the file is being played, the images are showed one after the other. They change many times per second so it makes the illusion of smooth video. There are different ways to code the video into the digital form so the need arises to convert the files from one format to other. The program which codes the analog video data to digital or decodes it the other way around is called codec. Some of the well known codecs are for example \textit{H.264} or \textit{MPEG-2}. In order to convert the file, each frame has to be re-encoded, which takes quite a big amount of time. This makes the video files ideal for use of our framework.

Before we use it, some more details have to be revealed. Because we usually want to not only watch the video but also listen to some sound, the videos are usually accompanied by one or more audio files and they are packed together in some container. When we want to convert the video, the container has to be opened, the video extracted, processed and stored again. The audio files are not in our concern and we will simply copy them. Common container formats are \textit{avi} or\textit{mkv}.

There is another important thing. In order to reduce file size and decrease the requirements of rendering the video, the file actually contains only few frames which carry the whole information. These are usually referred to as the key frames. The rest contains only information how the image differs from the previous one. This approach greatly reduces the file size while preserves good ability to recover from errors, e.g. when some part of the file is damaged. Consequence of this fact is, that we cannot split the video file wherever we want, because if the chunk started with some non-key frame, the conversion would not be done properly.

The problematic of video encoding is quite complex and used algorithms are very sophisticated and advanced, so we did not reinvent the wheel and used third party software for the video encoding, namely the ffmpeg and ffprobe from the FFmpeg project. For the sake of simplicity, the program outputs always video files packed in the Matroska container. The choice of the codec is arbitrary, however it is restricted by the list of options. All of this software is free and published under the GNU General Public License or GNU Lesser General Public License in case of FFmpeg.

\subsection*{Terminology}
To prevent misunderstandings there is a short list of frequently used terms:
\begin{itemize}
\item \textit{master}(or \textit{initiating node, initiator}) is a node which has work to do, i.e. file to encode. It initiates and controls the process.
\item \textit{task}(\textit{job}) refers to work that should be done, specified by the initiator.
\item \textit{chunk} denotes one part of the splitted file, which is supposed to be encoded independently.
\item \textit{(computing) node} is one particular entity which communicates with the master and encodes chunks for it. Theoretically, there can be more such entities on one physical computer, because each node is determined by its communicating port and address.
\item \textit{neighbor} is considered every node which is in the database of the node we talk about.
\end{itemize}

\subsection*{Thesis organization}
In the first chapter is given overview of the existing technologies and approaches and our framework is described. Second chapter describes implementation details. Third chapter introduces some experiments and summarizes the results. The fourth chapter is dedicated to alternatives and possible future improvements. In the appendix is then described installation and use of the program.