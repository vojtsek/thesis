# Problems, alternatives and possible improvements
Some alternative approaches were also considered during the designing. One of them was not to use periodical checking at all. It was based on the idea, that there would exist permanent connection with each neighbor and the state changes would be indicated by the events related to this connection. However it was rejected due to the requirements connected with keeping the connection. Furthermore, the changes of ready state of the node would have to be checked either periodically or the node would have to inform all its neighbors (even potential) about each change which would lead to another problems to deal with. It is in contrast with passivity of the slaves too.

Another issue was related to more sophisticated way of distributing the chunks. Namely some kind of hierarchy was considered in which the chunk references would be distributed to neighbors in packs. At the neighbor it would be further split and distributed among neighbor's neighbors and so on. A kind of a tree structure would be formed this way. The transfer of the file would be processed directly between the initiator and the leaf node. However this approach turned out to be complicated and brings many problems. For example in case of failure of some node which is high in the hierarchy a lot of chunks would have to be re-distributed. Also the initiator's ability to control the distribution would be reduced. Moreover, the advantages of this approach are not so significant at all, because the biggest portion of time is spent during transfers and processing the chunks. The time spent with distribution of references is not important at all.

Interesting alternative would be use of the anycast\footnote{https://en.wikipedia.org/wiki/Anycast} mechanism provided by the IPv6 protocol. The nodes would be addressed with an anycast address and each chunk would be simply sent to this address. Obvious disadvantage of this approach is the loss of the control of the distribution. More sophisticated variant would use special node which would maintain the list of free nodes and schedule the distribution. However this would break the peer to peer paradigm because of the centralization of control to one specific node.

Maybe the biggest unnecessary delay could appear when the whole process waits for some lost chunk. This is partially solved by resending chunks after the timeout. Also some form of redundancy could help, which on the other hand would certainly affect the effectiveness. If some node is processing more tasks sequentially while using approximately the same set of neighbors, the framework could also determine optimal chunk size to achieve good ratio of transfer and computing times. The chunks should not be too small because of the delays tied with its distribution. Optimal chunk size could significantly reduce the possible delay caused by waiting for re-encoding of some lost chunk. We could see at the end of the \hyperref[interpreting-the-results]{chapter 3}, that use of smaller chunk size leads to better performance, especially when the failure of some node occurs, the difference can be quite big.

Also the current implementation creates a separate connection for each data transfer. Alternatively, each chunk could be delivered and returned using the same connection which would lessen the demands of the communication. The connection's termination would also indicate problem with the chunk's processing or the neighbor itself. But the connection termination does not necessarily mean the failure of the process. Because it is desirable to avoid needless re-encoding of the chunks, this situation would has to be treated specially which would introduce additional problems. Also this approach does not fit very well to the current design in which each logical action is executed as the sequence of commands and for each sequence there is a special connection.